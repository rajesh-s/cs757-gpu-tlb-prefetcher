@inproceedings{valkyrie,
author = {Baruah, Trinayan and Sun, Yifan and Mojumder, Saiful A. and Abell\'{a}n, Jos\'{e} L. and Ukidave, Yash and Joshi, Ajay and Rubin, Norman and Kim, John and Kaeli, David},
title = {Valkyrie: Leveraging Inter-TLB Locality to Enhance GPU Performance},
year = {2020},
isbn = {9781450380751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410463.3414639},
doi = {10.1145/3410463.3414639},
booktitle = {Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques},
pages = {455–466},
numpages = {12},
keywords = {virtual memory, gpu, tlb prefetching, tlb design, tlb probing},
location = {Virtual Event, GA, USA},
series = {PACT '20}
}

@article{10.1145/2654822.2541942,
author = {Pichai, Bharath and Hsu, Lisa and Bhattacharjee, Abhishek},
title = {Architectural Support for Address Translation on GPUs: Designing Memory Management Units for CPU/GPUs with Unified Address Spaces},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2654822.2541942},
doi = {10.1145/2654822.2541942},
abstract = {The proliferation of heterogeneous compute platforms, of which CPU/GPU is a prevalent example, necessitates a manageable programming model to ensure widespread adoption. A key component of this is a shared unified address space between the heterogeneous units to obtain the programmability benefits of virtual memory.To this end, we are the first to explore GPU Memory Management Units(MMUs) consisting of Translation Lookaside Buffers (TLBs) and page table walkers (PTWs) for address translation in unified heterogeneous systems. We show the performance challenges posed by GPU warp schedulers on TLBs accessed in parallel with L1 caches, which provide many well-known programmability benefits. In response, we propose modest TLB and PTW augmentations that recover most of the performance lost by introducing L1 parallel TLB access. We also show that a little TLB-awareness can make other GPU performance enhancements (e.g., cache-conscious warp scheduling and dynamic warp formation on branch divergence) feasible in the face of cache-parallel address translation, bringing overheads in the range deemed acceptable for CPUs (10-15% of runtime). We presume this initial design leaves room for improvement but anticipate that our bigger insight, that a little TLB-awareness goes a long way in GPUs, will spur further work in this fruitful area.},
journal = {SIGARCH Comput. Archit. News},
month = {feb},
pages = {743–758},
numpages = {16},
keywords = {unified address space, gpus, mmus, tlbs}
}

  

@inproceedings{gpuvm,
author = {Pichai, Bharath and Hsu, Lisa and Bhattacharjee, Abhishek},
title = {Architectural Support for Address Translation on GPUs: Designing Memory Management Units for CPU/GPUs with Unified Address Spaces},
year = {2014},
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541940.2541942},
doi = {10.1145/2541940.2541942},
pages = {743–758},
numpages = {16},
keywords = {tlbs, gpus, mmus, unified address space},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}

@INPROCEEDINGS{mcm-gpu,
  author={Arunkumar, Akhil and Bolotin, Evgeny and Cho, Benjamin and Milic, Ugljesa and Ebrahimi, Eiman and Villa, Oreste and Jaleel, Aamer and Wu, Carole-Jean and Nellans, David},
  booktitle={2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={MCM-GPU: Multi-chip-module GPUs for continued performance scalability}, 
  year={2017},
  volume={},
  number={},
  pages={320-332},
  doi={10.1145/3079856.3080231}
}

@inproceedings{sohi,
  title={Reducing GPU Address Translation Overhead with Virtual Caching},
  author={Hongil Yoon and Jason Lowe-Power and Gurindar S. Sohi},
  year={2016}
}



  



  



